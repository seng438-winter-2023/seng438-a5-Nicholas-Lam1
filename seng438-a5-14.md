**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group 14:                         |
|-----------------------------------|
| Student 1: Nicholas               |   
| Student 2: Shahdad                |   
| Student 3: Hamza                  |   
| Student 4: Jaron                  |

# Introduction
This lab includes analysis of integration test data using reliability assessment tools. There are two ways to assess failure data:

1. Reliability growth testing
2. Reliability assessment using Reliability Demonstration Chart (RDC)

For both of above parts of this lab, we have utilized SRTAT to generate required outputs of this assignment.

# 

# Assessment Using Reliability Growth Testing 
As mentioned in the introduction portion of this assignment, we have utlizied SRTAT to generate required outputs. For this portion, we chose to use provided "Failure Report 10.docx" file and after converting its data to a text file and by importing to SRTAT and using Littlewood and Varral's Bayesian Reliability model plus Trend Analysis, Below outputs are generated:

Prediction:
![image](https://user-images.githubusercontent.com/115381298/227749659-407e39cd-8b60-429f-9cc9-5bbd3dfe0f5d.png)
Trend for above input file with 15 iterations is (failure numbers 14~28):
![image](https://user-images.githubusercontent.com/115381298/227749694-638edb8e-9974-4bf5-9d07-894230268b95.png)





# Assessment Using Reliability Demonstration Chart 
As mentioned in the introduction portion of this assignment, we have utlizied SRTAT to generate required outputs. For this portion, we chose to use provided "Failure Report 10.docx" file and after converting its data to a text file and by importing to SRTAT below reliablility demo charts are generated. Its important to note that for all 3 charts, Discrimination ratio, Customer risk and Developer risks are kept constant and only number of failures per second is modified:

Result with 5 failures per 1000 seconds:
![image](https://user-images.githubusercontent.com/115381298/227749952-9776f9ab-14b2-4f11-aa38-1bb53d1f5368.png)

Result with doubling of number of failures per second (10 failures per 1000 seconds):
![image](https://user-images.githubusercontent.com/115381298/227749971-9c696aff-6e11-43b3-87d0-0edddb1e1b6d.png)

Result with halving of number of failures per second (2.5 failures per 1000 seconds):
![image](https://user-images.githubusercontent.com/115381298/227750043-5c234dcd-8c81-45e5-9a8e-fbd07e3f4dee.png)


# 

# Comparison of Results
From results generated by using Reliability Demonostration Chart, It can be observed that SUT can be well accepted with 10 failures withing 1000 seconds. Although, system can also be accepted with 5 failures per 1000 seconds too, it will not be accepted with 2.5 failures per 1000 seconds and testing must be perfromed to decide if the SUT will be accepted or rejected.

# Discussion on Similarity and Differences of the Two Techniques

# How the team work/effort was divided and managed

# 

# Difficulties encountered, challenges overcome, and lessons learned

# Comments/feedback on the lab itself
