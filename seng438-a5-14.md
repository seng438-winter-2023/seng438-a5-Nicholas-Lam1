**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group 14:                         |
|-----------------------------------|
| Student 1: Nicholas               |   
| Student 2: Shahdad                |   
| Student 3: Hamza                  |   
| Student 4: Jaron                  |

# Introduction
This lab includes analysis of integration test data using reliability assessment tools. There are two ways to assess failure data:

1. Reliability growth testing
2. Reliability assessment using Reliability Demonstration Chart (RDC)

For both of above parts of this lab, we have utilized SRTAT to generate required outputs of this assignment.

# 

# Assessment Using Reliability Growth Testing 
As mentioned in the introduction portion of this assignment, we have utlizied SRTAT to generate required outputs. For this portion, we chose to use provided "Failure Report 10.docx" file and after converting its data to a text file and by importing to SRTAT and using Littlewood and Varral's Bayesian Reliability model plus Trend Analysis, Below outputs are generated:

### Prediction:
![image](https://user-images.githubusercontent.com/115381298/227749659-407e39cd-8b60-429f-9cc9-5bbd3dfe0f5d.png)


### Trend for above input file with 15 iterations is (failure numbers 14~28):
![image](https://user-images.githubusercontent.com/115381298/227749694-638edb8e-9974-4bf5-9d07-894230268b95.png)





# Assessment Using Reliability Demonstration Chart 
As mentioned in the introduction portion of this assignment, we have utilized SRTAT to generate required outputs. Like with the reliability growth testing, we used "Failure Report 10.docx". Once our converted data was uploaded, we generated our RDCs which can be seen below. Its important to note that for all 3 charts, Discrimination ratio, Customer risk and Developer risks are kept constant and only number of failures per second is modified:

### Result with minimum acceptable MTTFmin (3.85 failures per 1000 seconds):
![image](https://user-images.githubusercontent.com/101242454/228880389-3a6074b8-b997-48db-921b-ec8678bf4551.png)


### Result with doubling of number of failures per second (7.7 failures per 1000 seconds):
![image](https://user-images.githubusercontent.com/101242454/228881238-d83411c2-57ea-4f30-8997-b212643cb8a4.png)


### Result with halving of number of failures per second (1.925 failures per 1000 seconds):
![image](https://user-images.githubusercontent.com/101242454/228881408-7f872dbd-1ff4-4880-94ed-56bd14383321.png)


MTTFmin was determined through a process of trial and error. Different values for the "failures per second" field were selected until the SUT fell just beyond the acceptance margin into the acceptable region. This yielded the result of 3.85 failures per 1000 seconds as the MTTFmin. Then is was simply a matter of multiplying and dividing MTTFmin by 2 to get the other two charts.

# Comparison of Results
From results generated by using Reliability Demonstration Chart, it can be observed that the SUT can be well accepted with 7.7 failures per 1000 seconds. The system can just barely be accepted with 3.85 failures per 1000 seconds, but it will not be accepted with 1.925 failures per 1000 seconds and furthur testing must be performed to decide if the SUT will be accepted or rejected.

# Discussion on Similarity and Differences of the Two Techniques

# How the team work/effort was divided and managed

# 

# Difficulties encountered, challenges overcome, and lessons learned

# Comments/feedback on the lab itself
